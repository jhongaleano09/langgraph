{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c304f4",
   "metadata": {},
   "source": [
    "# Text-to-Report Chatbot Development Notebook\n",
    "\n",
    "Este notebook implementa el sistema completo de chatbot Text-to-Report usando LangGraph con un flujo de trabajo multiagente que incluye:\n",
    "\n",
    "- **Agente SQL**: Conversi√≥n de lenguaje natural a SQL\n",
    "- **Agente de Visualizaci√≥n**: Generaci√≥n inteligente de gr√°ficos\n",
    "- **Agente QA**: Validaci√≥n de calidad y coherencia\n",
    "- **Motor PDF**: Ensamblaje de reportes profesionales\n",
    "\n",
    "## Arquitectura del Sistema\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Usuario] -->|Consulta Natural| B[LangGraph Workflow]\n",
    "    B --> C[Agente SQL]\n",
    "    C --> D[Agente Visualizaci√≥n]\n",
    "    D --> E[Agente QA]\n",
    "    E -->|Rechazado| C\n",
    "    E -->|Aprobado| F[Motor PDF]\n",
    "    F --> G[Reporte Final]\n",
    "```\n",
    "\n",
    "**Objetivo**: Demostrar la funcionalidad completa del sistema desde consulta natural hasta reporte PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc07ab4",
   "metadata": {},
   "source": [
    "# 1. Environment Setup and Dependencies\n",
    "\n",
    "Instalaci√≥n e importaci√≥n de todas las librer√≠as necesarias para el sistema multiagente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, TypedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Environment and configuration\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# LangGraph and LangChain\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, END\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    print(\"‚úÖ LangGraph imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing LangGraph: {e}\")\n",
    "    print(\"Install with: pip install langgraph langchain-openai\")\n",
    "\n",
    "# Database and SQL\n",
    "try:\n",
    "    import sqlalchemy\n",
    "    from sqlalchemy import create_engine, text\n",
    "    import psycopg2\n",
    "    print(\"‚úÖ Database imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing database libraries: {e}\")\n",
    "    print(\"Install with: pip install sqlalchemy psycopg2-binary\")\n",
    "\n",
    "# Visualization\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"‚úÖ Visualization imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing visualization libraries: {e}\")\n",
    "    print(\"Install with: pip install plotly matplotlib seaborn pandas numpy\")\n",
    "\n",
    "# PDF Generation\n",
    "try:\n",
    "    from weasyprint import HTML, CSS\n",
    "    from jinja2 import Environment, Template\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    print(\"‚úÖ PDF generation imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing PDF libraries: {e}\")\n",
    "    print(\"Install with: pip install weasyprint jinja2\")\n",
    "\n",
    "# Additional utilities\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\nüéØ All core dependencies loaded successfully!\")\n",
    "print(f\"üìÖ Notebook initialized at: {datetime.now()}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afad38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "class Settings:\n",
    "    \"\"\"Simple settings class for the notebook\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # LLM Configuration\n",
    "        self.openai_api_key = os.getenv('OPENAI_API_KEY', 'your-key-here')\n",
    "        self.model_name = 'gpt-4'\n",
    "        \n",
    "        # Database Configuration (for demo, we'll use SQLite)\n",
    "        self.database_url = \"sqlite:///demo_data.db\"  # Simple SQLite for demo\n",
    "        \n",
    "        # Paths\n",
    "        self.temp_dir = Path(\"../temp\")\n",
    "        self.output_dir = Path(\"../generated_reports\") \n",
    "        \n",
    "        # Create directories\n",
    "        self.temp_dir.mkdir(exist_ok=True)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Limits\n",
    "        self.max_iterations = 3\n",
    "        self.default_limit = 100\n",
    "        \n",
    "        print(f\"‚öôÔ∏è Configuration loaded\")\n",
    "        print(f\"üîë OpenAI Key: {'‚úÖ Set' if self.openai_api_key != 'your-key-here' else '‚ùå Not set'}\")\n",
    "        print(f\"üóÑÔ∏è Database: {self.database_url}\")\n",
    "        print(f\"üìÅ Temp dir: {self.temp_dir}\")\n",
    "        print(f\"üìÑ Output dir: {self.output_dir}\")\n",
    "\n",
    "# Initialize settings\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ccfa32",
   "metadata": {},
   "source": [
    "# 2. Database Connection and Schema Loading\n",
    "\n",
    "Establecemos conexi√≥n con la base de datos y creamos datos de muestra para demostraci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo database and sample data\n",
    "def create_demo_database():\n",
    "    \"\"\"Creates a demo database with sample business data\"\"\"\n",
    "    \n",
    "    engine = create_engine(settings.database_url)\n",
    "    \n",
    "    # Create tables\n",
    "    with engine.connect() as conn:\n",
    "        # Drop existing tables\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS ventas\"))\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS productos\"))\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS regiones\"))\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS clientes\"))\n",
    "        \n",
    "        # Create schema\n",
    "        create_tables_sql = \"\"\"\n",
    "        CREATE TABLE regiones (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            nombre VARCHAR(50) NOT NULL,\n",
    "            pais VARCHAR(50) NOT NULL\n",
    "        );\n",
    "        \n",
    "        CREATE TABLE productos (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            nombre VARCHAR(100) NOT NULL,\n",
    "            categoria VARCHAR(50) NOT NULL,\n",
    "            precio DECIMAL(10,2) NOT NULL\n",
    "        );\n",
    "        \n",
    "        CREATE TABLE clientes (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            nombre VARCHAR(100) NOT NULL,\n",
    "            email VARCHAR(100),\n",
    "            region_id INTEGER,\n",
    "            fecha_registro DATE,\n",
    "            FOREIGN KEY (region_id) REFERENCES regiones(id)\n",
    "        );\n",
    "        \n",
    "        CREATE TABLE ventas (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            cliente_id INTEGER NOT NULL,\n",
    "            producto_id INTEGER NOT NULL,\n",
    "            cantidad INTEGER NOT NULL,\n",
    "            monto DECIMAL(10,2) NOT NULL,\n",
    "            fecha DATE NOT NULL,\n",
    "            vendedor VARCHAR(50),\n",
    "            FOREIGN KEY (cliente_id) REFERENCES clientes(id),\n",
    "            FOREIGN KEY (producto_id) REFERENCES productos(id)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        for statement in create_tables_sql.split(';'):\n",
    "            if statement.strip():\n",
    "                conn.execute(text(statement))\n",
    "        \n",
    "        # Insert sample data\n",
    "        \n",
    "        # Regiones\n",
    "        regiones_data = [\n",
    "            (1, 'Norte', 'Colombia'),\n",
    "            (2, 'Centro', 'Colombia'),\n",
    "            (3, 'Sur', 'Colombia'),\n",
    "            (4, 'Costa', 'Colombia')\n",
    "        ]\n",
    "        \n",
    "        for region in regiones_data:\n",
    "            conn.execute(text(\n",
    "                \"INSERT INTO regiones (id, nombre, pais) VALUES (?, ?, ?)\"\n",
    "            ), region)\n",
    "        \n",
    "        # Productos\n",
    "        productos_data = [\n",
    "            (1, 'Laptop Dell XPS', 'Electr√≥nicos', 2500000),\n",
    "            (2, 'iPhone 15', 'Electr√≥nicos', 3200000),\n",
    "            (3, 'Escritorio Ergon√≥mico', 'Muebles', 800000),\n",
    "            (4, 'Silla Ejecutiva', 'Muebles', 450000),\n",
    "            (5, 'Monitor 27\"', 'Electr√≥nicos', 900000),\n",
    "            (6, 'Teclado Mec√°nico', 'Electr√≥nicos', 320000),\n",
    "            (7, 'Mesa de Reuniones', 'Muebles', 1200000),\n",
    "            (8, 'Proyector 4K', 'Electr√≥nicos', 1800000)\n",
    "        ]\n",
    "        \n",
    "        for producto in productos_data:\n",
    "            conn.execute(text(\n",
    "                \"INSERT INTO productos (id, nombre, categoria, precio) VALUES (?, ?, ?, ?)\"\n",
    "            ), producto)\n",
    "        \n",
    "        # Clientes\n",
    "        import random\n",
    "        from datetime import date, timedelta\n",
    "        \n",
    "        nombres_clientes = [\n",
    "            'Empresa ABC', 'Corporaci√≥n XYZ', 'Startup Tech', 'Consultora Pro',\n",
    "            'Industrias Futuro', 'Comercial √âxito', 'Servicios Prime', 'Grupo Innovar',\n",
    "            'Tecnolog√≠a Avanzada', 'Soluciones Integrales', 'Desarrollo Agil', 'Sistemas Modernos'\n",
    "        ]\n",
    "        \n",
    "        for i, nombre in enumerate(nombres_clientes, 1):\n",
    "            region_id = random.choice([1, 2, 3, 4])\n",
    "            fecha_registro = date.today() - timedelta(days=random.randint(30, 365))\n",
    "            email = f\"{nombre.lower().replace(' ', '')}@email.com\"\n",
    "            \n",
    "            conn.execute(text(\n",
    "                \"INSERT INTO clientes (id, nombre, email, region_id, fecha_registro) VALUES (?, ?, ?, ?, ?)\"\n",
    "            ), (i, nombre, email, region_id, fecha_registro.isoformat()))\n",
    "        \n",
    "        # Ventas (√∫ltimos 6 meses)\n",
    "        venta_id = 1\n",
    "        start_date = date.today() - timedelta(days=180)\n",
    "        vendedores = ['Ana Garc√≠a', 'Carlos L√≥pez', 'Mar√≠a Rodriguez', 'Juan P√©rez', 'Laura Mart√≠n']\n",
    "        \n",
    "        for days_offset in range(180):\n",
    "            current_date = start_date + timedelta(days=days_offset)\n",
    "            # 1-5 ventas por d√≠a\n",
    "            num_ventas = random.randint(1, 5)\n",
    "            \n",
    "            for _ in range(num_ventas):\n",
    "                cliente_id = random.randint(1, len(nombres_clientes))\n",
    "                producto_id = random.randint(1, 8)\n",
    "                cantidad = random.randint(1, 10)\n",
    "                \n",
    "                # Get product price\n",
    "                precio_result = conn.execute(text(\n",
    "                    \"SELECT precio FROM productos WHERE id = ?\"\n",
    "                ), (producto_id,)).fetchone()\n",
    "                precio = precio_result[0]\n",
    "                \n",
    "                monto = precio * cantidad\n",
    "                vendedor = random.choice(vendedores)\n",
    "                \n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO ventas (id, cliente_id, producto_id, cantidad, monto, fecha, vendedor) \n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\"), (venta_id, cliente_id, producto_id, cantidad, monto, current_date.isoformat(), vendedor))\n",
    "                \n",
    "                venta_id += 1\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"‚úÖ Demo database created successfully!\")\n",
    "    \n",
    "    # Show sample data\n",
    "    with engine.connect() as conn:\n",
    "        # Count records\n",
    "        counts = {}\n",
    "        for table in ['regiones', 'productos', 'clientes', 'ventas']:\n",
    "            result = conn.execute(text(f\"SELECT COUNT(*) FROM {table}\")).fetchone()\n",
    "            counts[table] = result[0]\n",
    "        \n",
    "        print(f\"\\nüìä Sample data created:\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"  {table}: {count:,} records\")\n",
    "    \n",
    "    return engine\n",
    "\n",
    "# Create the demo database\n",
    "engine = create_demo_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract database schema and metadata\n",
    "def get_database_schema():\n",
    "    \"\"\"Extract DDL and metadata for LLM context\"\"\"\n",
    "    \n",
    "    # Get table info for SQLite\n",
    "    schema_info = []\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        # Get all tables\n",
    "        tables_result = conn.execute(text(\"\"\"\n",
    "            SELECT name FROM sqlite_master \n",
    "            WHERE type='table' AND name NOT LIKE 'sqlite_%'\n",
    "            ORDER BY name\n",
    "        \"\"\"))\n",
    "        tables = [row[0] for row in tables_result]\n",
    "        \n",
    "        schema_parts = []\n",
    "        \n",
    "        for table_name in tables:\n",
    "            # Get table schema\n",
    "            schema_result = conn.execute(text(f\"PRAGMA table_info({table_name})\"))\n",
    "            columns = schema_result.fetchall()\n",
    "            \n",
    "            # Build CREATE TABLE statement\n",
    "            create_table = f\"CREATE TABLE {table_name} (\"\n",
    "            column_defs = []\n",
    "            \n",
    "            for col in columns:\n",
    "                cid, name, type_, notnull, dflt_value, pk = col\n",
    "                col_def = f\"  {name} {type_}\"\n",
    "                if notnull:\n",
    "                    col_def += \" NOT NULL\"\n",
    "                if pk:\n",
    "                    col_def += \" PRIMARY KEY\"\n",
    "                if dflt_value:\n",
    "                    col_def += f\" DEFAULT {dflt_value}\"\n",
    "                column_defs.append(col_def)\n",
    "            \n",
    "            create_table += \"\\n\" + \",\\n\".join(column_defs) + \"\\n);\"\n",
    "            schema_parts.append(f\"-- Table: {table_name}\")\n",
    "            schema_parts.append(create_table)\n",
    "            \n",
    "            # Get sample data\n",
    "            sample_result = conn.execute(text(f\"SELECT * FROM {table_name} LIMIT 3\"))\n",
    "            sample_data = sample_result.fetchall()\n",
    "            if sample_data:\n",
    "                column_names = [col[0] for col in sample_result.description]\n",
    "                schema_parts.append(f\"-- Sample data for {table_name}:\")\n",
    "                schema_parts.append(f\"-- Columns: {', '.join(column_names)}\")\n",
    "                for i, row in enumerate(sample_data):\n",
    "                    schema_parts.append(f\"-- Row {i+1}: {dict(zip(column_names, row))}\")\n",
    "            \n",
    "            schema_parts.append(\"\")  # Empty line\n",
    "    \n",
    "    full_schema = \"\\n\".join(schema_parts)\n",
    "    \n",
    "    print(\"üìã Database schema extracted:\")\n",
    "    print(f\"  Tables: {len(tables)}\")\n",
    "    print(f\"  Schema length: {len(full_schema):,} characters\")\n",
    "    \n",
    "    return full_schema, tables\n",
    "\n",
    "# Extract schema\n",
    "database_schema, table_names = get_database_schema()\n",
    "\n",
    "# Show first part of schema\n",
    "print(\"\\nüìù Schema preview (first 1000 chars):\")\n",
    "print(\"=\" * 50)\n",
    "print(database_schema[:1000] + \"...\" if len(database_schema) > 1000 else database_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a94d4a",
   "metadata": {},
   "source": [
    "# 3. LangGraph State Definition\n",
    "\n",
    "Definimos la estructura de estado que gestiona todo el flujo de procesamiento multiagente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our LangGraph workflow\n",
    "class ReportState(TypedDict):\n",
    "    \"\"\"\n",
    "    State structure for the Text-to-Report workflow\n",
    "    Manages all data flowing between agents\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    user_query: str\n",
    "    user_profile: Optional[Dict[str, Any]]\n",
    "    \n",
    "    # SQL Processing\n",
    "    sql_query: Optional[str]\n",
    "    sql_explanation: Optional[str]\n",
    "    data_results: Optional[List[Dict[str, Any]]]\n",
    "    \n",
    "    # Visualization\n",
    "    chart_image: Optional[bytes]\n",
    "    chart_metadata: Optional[Dict[str, Any]]\n",
    "    \n",
    "    # QA Validation\n",
    "    qa_feedback: Optional[str]\n",
    "    qa_approved: bool\n",
    "    qa_score: Optional[float]\n",
    "    \n",
    "    # Iteration Control\n",
    "    iteration_count: int\n",
    "    max_iterations: int\n",
    "    \n",
    "    # Final Output\n",
    "    final_pdf: Optional[bytes]\n",
    "    report_id: Optional[str]\n",
    "    \n",
    "    # Metadata and Errors\n",
    "    timestamp: Optional[str]\n",
    "    errors: Optional[List[str]]\n",
    "    warnings: Optional[List[str]]\n",
    "\n",
    "def create_initial_state(user_query: str, user_profile: Optional[Dict] = None) -> ReportState:\n",
    "    \"\"\"Create initial state for a new report generation\"\"\"\n",
    "    return ReportState(\n",
    "        # Input\n",
    "        user_query=user_query,\n",
    "        user_profile=user_profile or {\"name\": \"Usuario Demo\"},\n",
    "        \n",
    "        # SQL Processing\n",
    "        sql_query=None,\n",
    "        sql_explanation=None,\n",
    "        data_results=None,\n",
    "        \n",
    "        # Visualization\n",
    "        chart_image=None,\n",
    "        chart_metadata=None,\n",
    "        \n",
    "        # QA Validation\n",
    "        qa_feedback=None,\n",
    "        qa_approved=False,\n",
    "        qa_score=None,\n",
    "        \n",
    "        # Iteration Control\n",
    "        iteration_count=0,\n",
    "        max_iterations=settings.max_iterations,\n",
    "        \n",
    "        # Final Output\n",
    "        final_pdf=None,\n",
    "        report_id=None,\n",
    "        \n",
    "        # Metadata\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        errors=[],\n",
    "        warnings=[]\n",
    "    )\n",
    "\n",
    "# Test state creation\n",
    "test_query = \"ventas totales del √∫ltimo mes por regi√≥n\"\n",
    "test_state = create_initial_state(test_query)\n",
    "\n",
    "print(\"‚úÖ State definition created\")\n",
    "print(f\"üß™ Test state for query: '{test_query}'\")\n",
    "print(f\"üìä State keys: {list(test_state.keys())}\")\n",
    "print(f\"üî¢ Max iterations: {test_state['max_iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950fd82",
   "metadata": {},
   "source": [
    "# 4. SQL Agent Implementation\n",
    "\n",
    "Agente especializado en convertir lenguaje natural a SQL con validaci√≥n y ejecuci√≥n segura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1829fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLAgent:\n",
    "    \"\"\"Agent for converting natural language to SQL queries\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=settings.model_name,\n",
    "            temperature=0.1,\n",
    "            api_key=settings.openai_api_key\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "        # Create the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "Eres un experto analista SQL especializado en convertir consultas en lenguaje natural a SQL preciso.\n",
    "\n",
    "ESQUEMA DE BASE DE DATOS:\n",
    "{database_schema}\n",
    "\n",
    "REGLAS ESTRICTAS:\n",
    "1. SOLO generar consultas SELECT\n",
    "2. Usar √öNICAMENTE las tablas del esquema proporcionado  \n",
    "3. Incluir LIMIT cuando sea apropiado para performance\n",
    "4. Usar JOINs correctos bas√°ndose en las relaciones FK\n",
    "5. Manejar fechas correctamente\n",
    "6. Usar alias descriptivos para columnas\n",
    "\n",
    "FORMATO DE RESPUESTA JSON:\n",
    "{{\n",
    "    \"sql_query\": \"SELECT ... FROM ... WHERE ...\",\n",
    "    \"explanation\": \"Explicaci√≥n clara de la consulta\",\n",
    "    \"tables_used\": [\"tabla1\", \"tabla2\"],\n",
    "    \"confidence_score\": 0.95\n",
    "}}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"\"\"\n",
    "Consulta del usuario: {user_query}\n",
    "\n",
    "Feedback de iteraci√≥n anterior (si aplica): {qa_feedback}\n",
    "\n",
    "Genera una consulta SQL precisa y optimizada.\n",
    "            \"\"\")\n",
    "        ])\n",
    "    \n",
    "    def validate_sql(self, sql_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Basic SQL validation\"\"\"\n",
    "        sql_upper = sql_query.upper().strip()\n",
    "        \n",
    "        errors = []\n",
    "        \n",
    "        # Check if it's a SELECT query\n",
    "        if not sql_upper.startswith('SELECT'):\n",
    "            errors.append(\"Solo se permiten consultas SELECT\")\n",
    "        \n",
    "        # Check for dangerous keywords\n",
    "        dangerous_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'TRUNCATE']\n",
    "        for keyword in dangerous_keywords:\n",
    "            if keyword in sql_upper:\n",
    "                errors.append(f\"Palabra clave prohibida: {keyword}\")\n",
    "        \n",
    "        # Add LIMIT if not present\n",
    "        if 'LIMIT' not in sql_upper and len(errors) == 0:\n",
    "            sql_query = sql_query.rstrip(';') + f' LIMIT {settings.default_limit};'\n",
    "        \n",
    "        return {\n",
    "            'valid': len(errors) == 0,\n",
    "            'errors': errors,\n",
    "            'safe_query': sql_query if len(errors) == 0 else None\n",
    "        }\n",
    "    \n",
    "    def execute_sql(self, sql_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute SQL query safely\"\"\"\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(sql_query))\n",
    "                columns = list(result.keys())\n",
    "                rows = result.fetchall()\n",
    "                \n",
    "                # Convert to list of dictionaries\n",
    "                data = [dict(zip(columns, row)) for row in rows]\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'data': data,\n",
    "                    'row_count': len(data),\n",
    "                    'columns': columns\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'data': [],\n",
    "                'row_count': 0,\n",
    "                'columns': []\n",
    "            }\n",
    "    \n",
    "    async def process(self, state: ReportState) -> ReportState:\n",
    "        \"\"\"Process the SQL generation step\"\"\"\n",
    "        try:\n",
    "            print(f\"üîç SQL Agent processing query: {state['user_query']}\")\n",
    "            \n",
    "            # Prepare prompt\n",
    "            messages = self.prompt.format_messages(\n",
    "                database_schema=database_schema,\n",
    "                user_query=state['user_query'],\n",
    "                qa_feedback=state.get('qa_feedback', 'Sin feedback previo')\n",
    "            )\n",
    "            \n",
    "            # Generate SQL with LLM\n",
    "            response = await self.llm.ainvoke(messages)\n",
    "            \n",
    "            # Parse JSON response\n",
    "            try:\n",
    "                # Clean response if it has markdown formatting\n",
    "                content = response.content.strip()\n",
    "                if content.startswith('```json'):\n",
    "                    content = content[7:]\n",
    "                if content.endswith('```'):\n",
    "                    content = content[:-3]\n",
    "                \n",
    "                llm_result = json.loads(content.strip())\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"No se pudo parsear la respuesta del LLM como JSON\")\n",
    "            \n",
    "            # Validate SQL\n",
    "            validation = self.validate_sql(llm_result['sql_query'])\n",
    "            \n",
    "            if not validation['valid']:\n",
    "                state['errors'].append(f\"SQL inv√°lido: {', '.join(validation['errors'])}\")\n",
    "                return state\n",
    "            \n",
    "            # Execute SQL\n",
    "            execution = self.execute_sql(validation['safe_query'])\n",
    "            \n",
    "            if not execution['success']:\n",
    "                state['errors'].append(f\"Error ejecutando SQL: {execution['error']}\")\n",
    "                return state\n",
    "            \n",
    "            # Update state\n",
    "            state['sql_query'] = validation['safe_query']\n",
    "            state['sql_explanation'] = llm_result.get('explanation', '')\n",
    "            state['data_results'] = execution['data']\n",
    "            state['iteration_count'] += 1\n",
    "            \n",
    "            print(f\"‚úÖ SQL Agent completed: {execution['row_count']} rows returned\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in SQL Agent: {str(e)}\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            state['errors'].append(error_msg)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize SQL Agent\n",
    "sql_agent = SQLAgent()\n",
    "print(\"‚úÖ SQL Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64747078",
   "metadata": {},
   "source": [
    "# 5. Visualization Agent Implementation\n",
    "\n",
    "Agente especializado en generar visualizaciones inteligentes bas√°ndose en los datos obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationAgent:\n",
    "    \"\"\"Agent for generating intelligent data visualizations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=settings.model_name,\n",
    "            temperature=0.1,\n",
    "            api_key=settings.openai_api_key\n",
    "        )\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "Eres un experto en visualizaci√≥n de datos especializado en an√°lisis autom√°tico de datasets y selecci√≥n √≥ptima de gr√°ficos.\n",
    "\n",
    "TIPOS DE GR√ÅFICOS DISPONIBLES:\n",
    "1. bar - Barras verticales (comparaciones categ√≥ricas)\n",
    "2. bar_horizontal - Barras horizontales (nombres largos)\n",
    "3. line - L√≠neas (tendencias temporales)\n",
    "4. pie - Circular (proporciones/porcentajes)\n",
    "5. scatter - Dispersi√≥n (correlaciones)\n",
    "6. heatmap - Mapa de calor (matrices de correlaci√≥n)\n",
    "7. histogram - Histograma (distribuciones)\n",
    "8. box - Caja y bigotes (distribuciones con outliers)\n",
    "\n",
    "REGLAS DE SELECCI√ìN:\n",
    "- Datos temporales: line chart\n",
    "- Categor√≠as comparativas: bar chart\n",
    "- Proporciones/partes del total: pie chart\n",
    "- Correlaciones num√©ricas: scatter plot\n",
    "- Matrices de datos: heatmap\n",
    "- Distribuciones: histogram o box plot\n",
    "\n",
    "FORMATO JSON RESPUESTA:\n",
    "{{\n",
    "    \"chart_type\": \"tipo_de_grafico\",\n",
    "    \"title\": \"T√≠tulo descriptivo\",\n",
    "    \"x_axis\": \"columna_x\",\n",
    "    \"y_axis\": \"columna_y\", \n",
    "    \"color_by\": \"columna_color_opcional\",\n",
    "    \"insights\": [\"insight1\", \"insight2\"],\n",
    "    \"confidence\": 0.95\n",
    "}}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"\"\"\n",
    "CONSULTA USUARIO: {user_query}\n",
    "SQL GENERADO: {sql_query}\n",
    "\n",
    "DATOS DISPONIBLES:\n",
    "Columnas: {columns}\n",
    "Filas: {row_count}\n",
    "Muestra de datos: {data_sample}\n",
    "\n",
    "Analiza los datos y recomienda la mejor visualizaci√≥n.\n",
    "            \"\"\")\n",
    "        ])\n",
    "    \n",
    "    def analyze_data_for_viz(self, data: List[Dict], columns: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze data structure to suggest appropriate visualizations\"\"\"\n",
    "        if not data:\n",
    "            return {\"can_visualize\": False, \"reason\": \"No data available\"}\n",
    "        \n",
    "        analysis = {\n",
    "            \"can_visualize\": True,\n",
    "            \"row_count\": len(data),\n",
    "            \"column_count\": len(columns),\n",
    "            \"column_types\": {}\n",
    "        }\n",
    "        \n",
    "        # Analyze column types\n",
    "        for col in columns:\n",
    "            sample_values = [row[col] for row in data[:10] if row[col] is not None]\n",
    "            \n",
    "            if not sample_values:\n",
    "                analysis[\"column_types\"][col] = \"null\"\n",
    "                continue\n",
    "            \n",
    "            # Check if numeric\n",
    "            try:\n",
    "                [float(str(v)) for v in sample_values]\n",
    "                analysis[\"column_types\"][col] = \"numeric\"\n",
    "            except (ValueError, TypeError):\n",
    "                # Check if date\n",
    "                try:\n",
    "                    from dateutil.parser import parse\n",
    "                    [parse(str(v)) for v in sample_values]\n",
    "                    analysis[\"column_types\"][col] = \"date\"\n",
    "                except:\n",
    "                    analysis[\"column_types\"][col] = \"categorical\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def create_visualization(self, chart_config: Dict, data: List[Dict]) -> str:\n",
    "        \"\"\"Create visualization using Plotly\"\"\"\n",
    "        try:\n",
    "            import plotly.graph_objects as go\n",
    "            import plotly.express as px\n",
    "            from datetime import datetime\n",
    "            \n",
    "            df_data = {col: [row.get(col) for row in data] for col in data[0].keys()}\n",
    "            \n",
    "            chart_type = chart_config[\"chart_type\"]\n",
    "            title = chart_config[\"title\"]\n",
    "            x_col = chart_config.get(\"x_axis\")\n",
    "            y_col = chart_config.get(\"y_axis\")\n",
    "            \n",
    "            fig = None\n",
    "            \n",
    "            if chart_type == \"bar\":\n",
    "                fig = px.bar(\n",
    "                    x=[str(val) for val in df_data[x_col]], \n",
    "                    y=df_data[y_col],\n",
    "                    title=title,\n",
    "                    labels={x_col: x_col, y_col: y_col}\n",
    "                )\n",
    "            \n",
    "            elif chart_type == \"line\":\n",
    "                fig = px.line(\n",
    "                    x=df_data[x_col], \n",
    "                    y=df_data[y_col],\n",
    "                    title=title,\n",
    "                    labels={x_col: x_col, y_col: y_col}\n",
    "                )\n",
    "            \n",
    "            elif chart_type == \"pie\":\n",
    "                fig = px.pie(\n",
    "                    values=df_data[y_col],\n",
    "                    names=[str(val) for val in df_data[x_col]],\n",
    "                    title=title\n",
    "                )\n",
    "            \n",
    "            elif chart_type == \"scatter\":\n",
    "                fig = px.scatter(\n",
    "                    x=df_data[x_col], \n",
    "                    y=df_data[y_col],\n",
    "                    title=title,\n",
    "                    labels={x_col: x_col, y_col: y_col}\n",
    "                )\n",
    "            \n",
    "            else:\n",
    "                # Fallback to bar chart\n",
    "                fig = px.bar(\n",
    "                    x=[str(val) for val in df_data[x_col]], \n",
    "                    y=df_data[y_col],\n",
    "                    title=title\n",
    "                )\n",
    "            \n",
    "            # Style the chart\n",
    "            fig.update_layout(\n",
    "                template=\"plotly_white\",\n",
    "                title_font_size=16,\n",
    "                showlegend=True,\n",
    "                height=500\n",
    "            )\n",
    "            \n",
    "            # Save as image\n",
    "            chart_path = f\"/tmp/chart_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "            fig.write_image(chart_path, format=\"png\", width=800, height=500)\n",
    "            \n",
    "            return chart_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating visualization: {e}\")\n",
    "            return None\n",
    "    \n",
    "    async def process(self, state: ReportState) -> ReportState:\n",
    "        \"\"\"Process the visualization generation step\"\"\"\n",
    "        try:\n",
    "            print(f\"üìä Visualization Agent processing data...\")\n",
    "            \n",
    "            if not state.get('data_results'):\n",
    "                state['errors'].append(\"No hay datos para visualizar\")\n",
    "                return state\n",
    "            \n",
    "            # Analyze data\n",
    "            data = state['data_results']\n",
    "            columns = list(data[0].keys()) if data else []\n",
    "            \n",
    "            data_analysis = self.analyze_data_for_viz(data, columns)\n",
    "            \n",
    "            if not data_analysis[\"can_visualize\"]:\n",
    "                state['errors'].append(f\"No se puede visualizar: {data_analysis['reason']}\")\n",
    "                return state\n",
    "            \n",
    "            # Prepare data sample for LLM\n",
    "            data_sample = data[:3] if len(data) >= 3 else data\n",
    "            \n",
    "            # Generate visualization config with LLM\n",
    "            messages = self.prompt.format_messages(\n",
    "                user_query=state['user_query'],\n",
    "                sql_query=state['sql_query'],\n",
    "                columns=columns,\n",
    "                row_count=len(data),\n",
    "                data_sample=json.dumps(data_sample, indent=2, default=str)\n",
    "            )\n",
    "            \n",
    "            response = await self.llm.ainvoke(messages)\n",
    "            \n",
    "            # Parse response\n",
    "            try:\n",
    "                content = response.content.strip()\n",
    "                if content.startswith('```json'):\n",
    "                    content = content[7:]\n",
    "                if content.endswith('```'):\n",
    "                    content = content[:-3]\n",
    "                \n",
    "                viz_config = json.loads(content.strip())\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"No se pudo parsear la configuraci√≥n de visualizaci√≥n\")\n",
    "            \n",
    "            # Create visualization\n",
    "            chart_path = self.create_visualization(viz_config, data)\n",
    "            \n",
    "            if not chart_path:\n",
    "                state['errors'].append(\"Error generando el gr√°fico\")\n",
    "                return state\n",
    "            \n",
    "            # Update state\n",
    "            state['visualization_config'] = viz_config\n",
    "            state['visualization_path'] = chart_path\n",
    "            state['insights'] = viz_config.get('insights', [])\n",
    "            \n",
    "            print(f\"‚úÖ Visualization Agent completed: {viz_config['chart_type']} chart created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in Visualization Agent: {str(e)}\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            state['errors'].append(error_msg)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize Visualization Agent  \n",
    "viz_agent = VisualizationAgent()\n",
    "print(\"‚úÖ Visualization Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d91d42",
   "metadata": {},
   "source": [
    "# 6. QA Agent Implementation\n",
    "\n",
    "Agente de validaci√≥n de calidad que revisa la coherencia entre consulta, datos y visualizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAAgent:\n",
    "    \"\"\"Quality Assurance Agent for validating query results\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=settings.model_name,\n",
    "            temperature=0.1,\n",
    "            api_key=settings.openai_api_key\n",
    "        )\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "Eres un analista de calidad especializado en validar la coherencia entre consultas de usuario, resultados SQL y visualizaciones.\n",
    "\n",
    "TU MISI√ìN:\n",
    "1. Verificar que la consulta SQL responde correctamente a la pregunta del usuario\n",
    "2. Validar que los datos obtenidos son relevantes y completos\n",
    "3. Confirmar que la visualizaci√≥n elegida es apropiada para los datos\n",
    "4. Identificar posibles problemas o inconsistencias\n",
    "5. Sugerir mejoras si es necesario\n",
    "\n",
    "CRITERIOS DE EVALUACI√ìN:\n",
    "- Relevancia: ¬øLos datos responden la pregunta?\n",
    "- Completitud: ¬øFalta informaci√≥n importante?\n",
    "- Precisi√≥n: ¬øLa SQL es correcta t√©cnicamente?\n",
    "- Visualizaci√≥n: ¬øEl gr√°fico representa bien los datos?\n",
    "- Insights: ¬øSe pueden extraer conclusiones √∫tiles?\n",
    "\n",
    "FORMATO JSON RESPUESTA:\n",
    "{{\n",
    "    \"quality_score\": 0.85,\n",
    "    \"is_satisfactory\": true,\n",
    "    \"evaluation\": {{\n",
    "        \"relevance\": {{\"score\": 0.9, \"comment\": \"Datos muy relevantes\"}},\n",
    "        \"completeness\": {{\"score\": 0.8, \"comment\": \"Podr√≠a incluir m√°s detalle\"}},\n",
    "        \"accuracy\": {{\"score\": 1.0, \"comment\": \"SQL correcta\"}},\n",
    "        \"visualization\": {{\"score\": 0.8, \"comment\": \"Gr√°fico apropiado\"}},\n",
    "        \"insights\": {{\"score\": 0.7, \"comment\": \"Insights b√°sicos\"}}\n",
    "    }},\n",
    "    \"feedback\": \"Descripci√≥n espec√≠fica de mejoras\",\n",
    "    \"requires_iteration\": false\n",
    "}}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"\"\"\n",
    "CONSULTA ORIGINAL: {user_query}\n",
    "\n",
    "SQL GENERADO: {sql_query}\n",
    "EXPLICACI√ìN SQL: {sql_explanation}\n",
    "\n",
    "DATOS OBTENIDOS:\n",
    "- Filas: {row_count}\n",
    "- Columnas: {columns}\n",
    "- Muestra: {data_sample}\n",
    "\n",
    "VISUALIZACI√ìN:\n",
    "- Tipo: {chart_type}\n",
    "- T√≠tulo: {chart_title}\n",
    "- Configuraci√≥n: {viz_config}\n",
    "\n",
    "INSIGHTS GENERADOS: {insights}\n",
    "\n",
    "Eval√∫a la calidad y coherencia de toda la pipeline.\n",
    "            \"\"\")\n",
    "        ])\n",
    "    \n",
    "    def calculate_quality_metrics(self, state: ReportState) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate basic quality metrics\"\"\"\n",
    "        metrics = {\n",
    "            \"has_data\": len(state.get('data_results', [])) > 0,\n",
    "            \"has_visualization\": state.get('visualization_path') is not None,\n",
    "            \"has_sql\": state.get('sql_query') is not None,\n",
    "            \"has_insights\": len(state.get('insights', [])) > 0,\n",
    "            \"error_count\": len(state.get('errors', []))\n",
    "        }\n",
    "        \n",
    "        # Basic quality score\n",
    "        score = 0.0\n",
    "        if metrics[\"has_data\"]: score += 0.3\n",
    "        if metrics[\"has_sql\"]: score += 0.2\n",
    "        if metrics[\"has_visualization\"]: score += 0.2\n",
    "        if metrics[\"has_insights\"]: score += 0.2\n",
    "        if metrics[\"error_count\"] == 0: score += 0.1\n",
    "        \n",
    "        metrics[\"basic_score\"] = min(score, 1.0)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    async def process(self, state: ReportState) -> ReportState:\n",
    "        \"\"\"Process the quality assurance step\"\"\"\n",
    "        try:\n",
    "            print(f\"üîç QA Agent validating results...\")\n",
    "            \n",
    "            # Calculate basic metrics\n",
    "            metrics = self.calculate_quality_metrics(state)\n",
    "            \n",
    "            # If basic quality is too low, mark as unsatisfactory\n",
    "            if metrics[\"basic_score\"] < 0.5:\n",
    "                state['qa_feedback'] = \"Calidad insuficiente: faltan componentes b√°sicos\"\n",
    "                state['quality_score'] = metrics[\"basic_score\"]\n",
    "                state['requires_iteration'] = True\n",
    "                return state\n",
    "            \n",
    "            # Prepare data for LLM evaluation\n",
    "            data_results = state.get('data_results', [])\n",
    "            viz_config = state.get('visualization_config', {})\n",
    "            \n",
    "            data_sample = data_results[:3] if len(data_results) >= 3 else data_results\n",
    "            columns = list(data_results[0].keys()) if data_results else []\n",
    "            \n",
    "            # Generate detailed QA evaluation\n",
    "            messages = self.prompt.format_messages(\n",
    "                user_query=state['user_query'],\n",
    "                sql_query=state.get('sql_query', 'No SQL generado'),\n",
    "                sql_explanation=state.get('sql_explanation', 'Sin explicaci√≥n'),\n",
    "                row_count=len(data_results),\n",
    "                columns=columns,\n",
    "                data_sample=json.dumps(data_sample, indent=2, default=str),\n",
    "                chart_type=viz_config.get('chart_type', 'No especificado'),\n",
    "                chart_title=viz_config.get('title', 'Sin t√≠tulo'),\n",
    "                viz_config=json.dumps(viz_config, indent=2, default=str),\n",
    "                insights=state.get('insights', [])\n",
    "            )\n",
    "            \n",
    "            response = await self.llm.ainvoke(messages)\n",
    "            \n",
    "            # Parse response\n",
    "            try:\n",
    "                content = response.content.strip()\n",
    "                if content.startswith('```json'):\n",
    "                    content = content[7:]\n",
    "                if content.endswith('```'):\n",
    "                    content = content[:-3]\n",
    "                \n",
    "                qa_result = json.loads(content.strip())\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"No se pudo parsear la evaluaci√≥n QA\")\n",
    "            \n",
    "            # Update state with QA results\n",
    "            state['quality_score'] = qa_result.get('quality_score', metrics[\"basic_score\"])\n",
    "            state['qa_feedback'] = qa_result.get('feedback', 'Sin feedback espec√≠fico')\n",
    "            state['requires_iteration'] = qa_result.get('requires_iteration', False)\n",
    "            state['qa_evaluation'] = qa_result.get('evaluation', {})\n",
    "            \n",
    "            # Determine if iteration is needed\n",
    "            is_satisfactory = qa_result.get('is_satisfactory', True)\n",
    "            quality_threshold = 0.7\n",
    "            \n",
    "            if not is_satisfactory or state['quality_score'] < quality_threshold:\n",
    "                state['requires_iteration'] = True\n",
    "                print(f\"‚ö†Ô∏è  QA Agent: Quality below threshold ({state['quality_score']:.2f}), iteration required\")\n",
    "            else:\n",
    "                state['requires_iteration'] = False\n",
    "                print(f\"‚úÖ QA Agent: Quality acceptable ({state['quality_score']:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in QA Agent: {str(e)}\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            state['errors'].append(error_msg)\n",
    "            state['requires_iteration'] = True\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize QA Agent\n",
    "qa_agent = QAAgent()\n",
    "print(\"‚úÖ QA Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a66a5d",
   "metadata": {},
   "source": [
    "# 7. LangGraph Workflow Integration\n",
    "\n",
    "Orquestaci√≥n completa del flujo de trabajo usando LangGraph con manejo de estados y decisiones condicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6193f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Literal\n",
    "\n",
    "# Define workflow nodes\n",
    "async def sql_node(state: ReportState) -> ReportState:\n",
    "    \"\"\"SQL generation node\"\"\"\n",
    "    return await sql_agent.process(state)\n",
    "\n",
    "async def visualization_node(state: ReportState) -> ReportState:\n",
    "    \"\"\"Visualization generation node\"\"\"\n",
    "    return await viz_agent.process(state)\n",
    "\n",
    "async def qa_node(state: ReportState) -> ReportState:\n",
    "    \"\"\"Quality assurance node\"\"\"\n",
    "    return await qa_agent.process(state)\n",
    "\n",
    "# Conditional routing logic\n",
    "def should_continue(state: ReportState) -> Literal[\"continue\", \"end\"]:\n",
    "    \"\"\"Determine if iteration is needed or workflow should end\"\"\"\n",
    "    \n",
    "    # Check for critical errors\n",
    "    if state.get('errors'):\n",
    "        print(f\"üö´ Workflow stopped due to errors: {state['errors']}\")\n",
    "        return \"end\"\n",
    "    \n",
    "    # Check iteration requirements\n",
    "    if state.get('requires_iteration', False):\n",
    "        # Limit iterations to prevent infinite loops\n",
    "        max_iterations = 3\n",
    "        if state.get('iteration_count', 0) >= max_iterations:\n",
    "            print(f\"üîÑ Max iterations reached ({max_iterations}), ending workflow\")\n",
    "            return \"end\"\n",
    "        \n",
    "        print(f\"üîÑ QA requires iteration {state.get('iteration_count', 0) + 1}\")\n",
    "        return \"continue\"\n",
    "    \n",
    "    print(\"‚úÖ Workflow completed successfully\")\n",
    "    return \"end\"\n",
    "\n",
    "def route_after_qa(state: ReportState) -> Literal[\"sql_agent\", \"pdf_generation\"]:\n",
    "    \"\"\"Route after QA: back to SQL if iteration needed, or to PDF if done\"\"\"\n",
    "    if should_continue(state) == \"continue\":\n",
    "        return \"sql_agent\"\n",
    "    else:\n",
    "        return \"pdf_generation\"\n",
    "\n",
    "async def pdf_generation_node(state: ReportState) -> ReportState:\n",
    "    \"\"\"PDF generation node (simplified for demo)\"\"\"\n",
    "    try:\n",
    "        print(\"üìÑ Generating PDF report...\")\n",
    "        \n",
    "        # Create a simple summary\n",
    "        summary = {\n",
    "            \"query\": state.get('user_query', 'No query'),\n",
    "            \"sql\": state.get('sql_query', 'No SQL'),\n",
    "            \"data_count\": len(state.get('data_results', [])),\n",
    "            \"visualization\": state.get('visualization_config', {}).get('chart_type', 'None'),\n",
    "            \"quality_score\": state.get('quality_score', 0.0),\n",
    "            \"insights\": state.get('insights', [])\n",
    "        }\n",
    "        \n",
    "        # For demo purposes, just store the summary\n",
    "        state['pdf_summary'] = summary\n",
    "        state['pdf_generated'] = True\n",
    "        \n",
    "        print(\"‚úÖ PDF generation completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in PDF generation: {str(e)}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        state['errors'].append(error_msg)\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Build the LangGraph workflow\n",
    "def create_workflow() -> StateGraph:\n",
    "    \"\"\"Create and configure the LangGraph workflow\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(ReportState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"sql_agent\", sql_node)\n",
    "    workflow.add_node(\"visualization_agent\", visualization_node) \n",
    "    workflow.add_node(\"qa_agent\", qa_node)\n",
    "    workflow.add_node(\"pdf_generation\", pdf_generation_node)\n",
    "    \n",
    "    # Define the flow\n",
    "    workflow.add_edge(START, \"sql_agent\")\n",
    "    workflow.add_edge(\"sql_agent\", \"visualization_agent\")\n",
    "    workflow.add_edge(\"visualization_agent\", \"qa_agent\")\n",
    "    \n",
    "    # Conditional routing after QA\n",
    "    workflow.add_conditional_edges(\n",
    "        \"qa_agent\",\n",
    "        route_after_qa,\n",
    "        {\n",
    "            \"sql_agent\": \"sql_agent\",  # Iterate back to SQL\n",
    "            \"pdf_generation\": \"pdf_generation\"  # Continue to PDF\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"pdf_generation\", END)\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# Compile the workflow\n",
    "workflow = create_workflow()\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ LangGraph workflow compiled and ready\")\n",
    "\n",
    "# Visualize the workflow (optional)\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    \n",
    "    # Generate workflow diagram\n",
    "    workflow_image = app.get_graph().draw_mermaid_png()\n",
    "    \n",
    "    # Display the diagram\n",
    "    display(Image(workflow_image))\n",
    "    print(\"üìä Workflow diagram displayed above\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  Install graphviz and pillow to see workflow diagram\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  Could not display diagram: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1c4f4",
   "metadata": {},
   "source": [
    "# 8. End-to-End Testing\n",
    "\n",
    "Vamos a probar el sistema completo con consultas de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_test_query(query: str, description: str = \"\"):\n",
    "    \"\"\"Run a test query through the complete pipeline\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üß™ TESTING: {description}\")\n",
    "    print(f\"üìù Query: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create initial state\n",
    "    initial_state = ReportState(\n",
    "        user_query=query,\n",
    "        sql_query=\"\",\n",
    "        sql_explanation=\"\",\n",
    "        data_results=[],\n",
    "        visualization_config={},\n",
    "        visualization_path=\"\",\n",
    "        insights=[],\n",
    "        qa_feedback=\"\",\n",
    "        quality_score=0.0,\n",
    "        requires_iteration=False,\n",
    "        iteration_count=0,\n",
    "        errors=[],\n",
    "        pdf_summary={},\n",
    "        pdf_generated=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Run the workflow\n",
    "        result = await app.ainvoke(initial_state)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìä RESULTS:\")\n",
    "        print(f\"‚úÖ SQL Generated: {bool(result.get('sql_query'))}\")\n",
    "        print(f\"‚úÖ Data Retrieved: {len(result.get('data_results', []))} rows\")\n",
    "        print(f\"‚úÖ Visualization: {result.get('visualization_config', {}).get('chart_type', 'None')}\")\n",
    "        print(f\"‚úÖ Quality Score: {result.get('quality_score', 0.0):.2f}\")\n",
    "        print(f\"‚úÖ PDF Generated: {result.get('pdf_generated', False)}\")\n",
    "        print(f\"‚ö†Ô∏è  Errors: {len(result.get('errors', []))}\")\n",
    "        \n",
    "        if result.get('errors'):\n",
    "            print(f\"‚ùå Error Details: {result['errors']}\")\n",
    "        \n",
    "        if result.get('insights'):\n",
    "            print(f\"üí° Insights: {result['insights']}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed with error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test Cases\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"¬øCu√°les son las ventas totales por regi√≥n?\",\n",
    "        \"description\": \"An√°lisis b√°sico de ventas por regi√≥n\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Muestra la evoluci√≥n de ventas de los √∫ltimos 3 meses\",\n",
    "        \"description\": \"An√°lisis temporal de tendencias\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"¬øQu√© productos son los m√°s vendidos?\",\n",
    "        \"description\": \"Ranking de productos top\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Compara las ventas entre Norte y Sur\",\n",
    "        \"description\": \"Comparativa entre regiones espec√≠ficas\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üöÄ Starting End-to-End Tests\")\n",
    "print(\"‚ö†Ô∏è  Make sure you have your OpenAI API key configured!\")\n",
    "print(\"\\nTo run tests, execute the following cells one by one...\")\n",
    "\n",
    "# Display test overview\n",
    "for i, test in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. {test['description']}: '{test['query']}'\")\n",
    "\n",
    "print(\"\\n‚ú® Ready to test the complete Text-to-Report pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db30a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic sales by region analysis\n",
    "await run_test_query(\n",
    "    query=\"¬øCu√°les son las ventas totales por regi√≥n?\",\n",
    "    description=\"An√°lisis b√°sico de ventas por regi√≥n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Temporal analysis  \n",
    "await run_test_query(\n",
    "    query=\"Muestra la evoluci√≥n de ventas de los √∫ltimos 3 meses\",\n",
    "    description=\"An√°lisis temporal de tendencias\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Product ranking\n",
    "await run_test_query(\n",
    "    query=\"¬øQu√© productos son los m√°s vendidos?\",\n",
    "    description=\"Ranking de productos top\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ce133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Comparative analysis\n",
    "await run_test_query(\n",
    "    query=\"Compara las ventas entre Norte y Sur\",\n",
    "    description=\"Comparativa entre regiones espec√≠ficas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744d6d7",
   "metadata": {},
   "source": [
    "# 9. Development Summary & Next Steps\n",
    "\n",
    "## ‚úÖ What We've Built\n",
    "\n",
    "This notebook demonstrates a complete **Text-to-Report** system using LangGraph that:\n",
    "\n",
    "1. **SQL Agent**: Converts natural language queries to secure SQL using OpenAI GPT-4\n",
    "2. **Visualization Agent**: Creates intelligent charts based on data analysis  \n",
    "3. **QA Agent**: Validates results quality and triggers iterations when needed\n",
    "4. **LangGraph Orchestration**: Manages the multi-agent workflow with conditional routing\n",
    "5. **Database Integration**: Works with PostgreSQL demo data (regions, products, clients, sales)\n",
    "\n",
    "## üèóÔ∏è Architecture Highlights\n",
    "\n",
    "- **Security**: SQL validation prevents dangerous operations\n",
    "- **Quality Control**: QA agent ensures coherent results before final output\n",
    "- **Iterative Improvement**: Failed quality checks trigger workflow iterations\n",
    "- **Error Handling**: Comprehensive error management throughout the pipeline\n",
    "- **Extensibility**: Modular design allows easy addition of new agents\n",
    "\n",
    "## üöÄ Next Development Steps\n",
    "\n",
    "1. **PDF Generation**: Implement full PDF report generation with WeasyPrint\n",
    "2. **API Integration**: Connect to the FastAPI service layer\n",
    "3. **Production Database**: Replace demo data with real business database\n",
    "4. **Advanced Visualizations**: Add more chart types and statistical analysis\n",
    "5. **User Authentication**: Add security and user management\n",
    "6. **Deployment**: Test on Azure VM environment\n",
    "\n",
    "## üîß Production Checklist\n",
    "\n",
    "- [ ] Configure OpenAI API key in environment variables\n",
    "- [ ] Install all required dependencies (`pip install -r requirements.txt`)\n",
    "- [ ] Set up PostgreSQL database with real data\n",
    "- [ ] Configure monitoring and logging\n",
    "- [ ] Deploy using Docker containers\n",
    "- [ ] Set up CI/CD pipeline\n",
    "\n",
    "## üìö Usage Instructions\n",
    "\n",
    "1. **Setup Environment**: Configure all dependencies and API keys\n",
    "2. **Run Tests**: Execute the test cells to validate functionality\n",
    "3. **Customize Queries**: Modify test queries for your specific use cases\n",
    "4. **Monitor Performance**: Check quality scores and iteration patterns\n",
    "5. **Scale Up**: Deploy to production when satisfied with results\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ This notebook serves as the foundation for your complete Text-to-Report solution!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
